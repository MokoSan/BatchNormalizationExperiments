{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "<img src=\"https://api.ning.com/files/EXPL4V-n0-S-UQNnNq6bext-hycLoK-u6aEjnY7J2UyCWgn3eFDfbFC0T*6wIFSowUo2bxbUThjv1YqkRXddrKjFeLP8ZXqE/N2.jpg\" width=\"400\" height=\"50\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of Batch Normalization is to reduce overall __Covariant Shift__ that is a result of changing parameters from the previous layers are constantly changing. The effect of utilizing Batch Normalization is the ability to use higher learning rates and be less careful about weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import metrics\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Activations Over Time\n",
    "\n",
    "### Data Set \n",
    "\n",
    "MNIST dataset\n",
    "\n",
    "### Neural Network Architecture:\n",
    "1. 3 Fully Connected Hidden Layers\n",
    "2. 100 Activations Per Hidden Layer\n",
    "3. Each Hidden Layer uses Sigmoid \n",
    "4. Weights initialized to small Gaussian Values\n",
    "5. Last Layer is connected to 10 Activation Layers and Cross Entropy\n",
    "\n",
    "### Training\n",
    "Training on 50,000 steps with 60 examples each per minibatch. \n",
    "\n",
    "### Experimental Observation \n",
    "Comparisons Made between Baseline [ Without Batch Norm ] and Batch Norm at Every Layer\n",
    "\n",
    "### Graphs\n",
    "1. Test Accuracy of the MNIST Network trained with and without Batch Normalization vs. the number of training steps.\n",
    "2. The evolution of input distributions to a typical sigmoid over the course of training shown at 15th, 50th and 85th Percentile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed( seed )\n",
    "\n",
    "# Initializing Hyperparameters\n",
    "NUM_EPOCHS  = 50000\n",
    "BATCH_COUNT = 60\n",
    "\n",
    "# Getting the MNIST Data\n",
    "( X_train, y_train ), ( X_test, y_test ) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_model( apply_batchnormalization = False ): \n",
    "    '''Function creates the model for the first experiment that has optionality\n",
    "       to enable / disable batch normalization. \n",
    "    '''\n",
    "\n",
    "    mnist_model = Input( )\n",
    "    \n",
    "    # Input Layer\n",
    "    mnist_model.add( Dense( units = 1000,\n",
    "                            input_shape = ( 28, 28, 1 ),\n",
    "                            activation  = K.sigmoid\n",
    "                            ))\n",
    "    \n",
    "    if ( apply_batchnormalization ):\n",
    "        mnist_model.add( BatchNormalization() )\n",
    "\n",
    "    # 3 Hidden Layers\n",
    "    for i in range( 3 ):\n",
    "        mnist_model.add( Dense( units              = 100,\n",
    "                                activation         = K.sigmoid,\n",
    "                                kernel_initializer = RandomNormal()))\n",
    "        if ( apply_batchnormalization ):\n",
    "            mnist_model.add( BatchNormalization() )\n",
    "            \n",
    "        \n",
    "    # Output Layer with 10 Units for each digit and a Softmax Activation\n",
    "    mnist_model.add(Dense( units = 10 , activation= K.softmax ))\n",
    "    \n",
    "    mnist_model.compile( loss = ''  metrics = [ metrics.categorical_accuracy ])\n",
    "    \n",
    "    return mnist_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: ImageNet Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
