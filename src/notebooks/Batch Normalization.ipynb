{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "<img src=\"https://api.ning.com/files/EXPL4V-n0-S-UQNnNq6bext-hycLoK-u6aEjnY7J2UyCWgn3eFDfbFC0T*6wIFSowUo2bxbUThjv1YqkRXddrKjFeLP8ZXqE/N2.jpg\" width=\"400\" height=\"50\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of Batch Normalization is to reduce overall __Covariant Shift__ that is a result of changing parameters from the previous layers are constantly changing. The effect of utilizing Batch Normalization is the ability to use higher learning rates and be less careful about weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshi/batchnorm/env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Activations Over Time\n",
    "\n",
    "### Data Set \n",
    "\n",
    "MNIST dataset\n",
    "\n",
    "### Neural Network Architecture:\n",
    "1. 3 Fully Connected Hidden Layers\n",
    "2. 100 Activations Per Hidden Layer\n",
    "3. Each Hidden Layer uses Sigmoid \n",
    "4. Weights initialized to small Gaussian Values\n",
    "5. Last Layer is connected to 10 Activation Layers and Cross Entropy\n",
    "\n",
    "### Training\n",
    "Training on 50,000 steps with 60 examples each per minibatch. \n",
    "\n",
    "### Experimental Observation \n",
    "Comparisons Made between Baseline [ Without Batch Norm ] and Batch Norm at Every Layer\n",
    "\n",
    "### Graphs\n",
    "1. Test Accuracy of the MNIST Network trained with and without Batch Normalization vs. the number of training steps.\n",
    "2. The evolution of input distributions to a typical sigmoid over the course of training shown at 15th, 50th and 85th Percentile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed( seed )\n",
    "\n",
    "# Initializing Hyperparameters\n",
    "NUM_EPOCHS  = 50000\n",
    "BATCH_COUNT = 60\n",
    "\n",
    "# Getting the MNIST Data\n",
    "mnist_classes = 10\n",
    "( X_train, y_train ), ( X_test, y_test ) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-0baa60d176c4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-0baa60d176c4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print ( f\"Shape of X Training Data: {X_train.shape}\" )\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Printing out the shape of the MNIST Data\n",
    "print ( f\"Shape of X Training Data: {X_train.shape}\" )\n",
    "print ( f\"Shape of Y Training Data: {y_train.shape}\" )\n",
    "\n",
    "# Plotting the first few images\n",
    "plt.rcParams['figure.figsize'] = (8,8)\n",
    "for i in range( 9 ):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.subplots_adjust( hspace=0.4, wspace=0.1 )\n",
    "    plt.imshow( X_train[i], cmap='gray' )\n",
    "    plt.title( f\"Class { y_train[i]}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_model( apply_batchnormalization = False ): \n",
    "    '''Function creates the model for the first experiment that has optionality\n",
    "       to enable / disable batch normalization. \n",
    "    '''\n",
    "    \n",
    "    input_layer = Input( shape = ( 28, 28, 1 ))\n",
    "    dense       = Dense( units = 1000,\n",
    "                         input_shape = ( 28, 28, 1 ),\n",
    "                         activation  = K.sigmoid )( input_layer )\n",
    "    \n",
    "    if ( apply_batchnormalization ):\n",
    "        dense  = BatchNormalization()( dense )\n",
    "\n",
    "    # 3 Hidden Layers\n",
    "    for i in range( 3 ):\n",
    "        dense =  Dense( units              = 100,\n",
    "                        activation         = K.sigmoid,\n",
    "                        kernel_initializer = RandomNormal() )( dense )\n",
    "        if ( apply_batchnormalization ):\n",
    "            dense =  BatchNormalization()( dense )\n",
    "            \n",
    "        \n",
    "    # Output Layer with 10 Units for each digit and a Softmax Activation \n",
    "    output = Dense( units = 10 , activation= K.softmax )( dense )\n",
    "\n",
    "    model = Model( inputs = input_layer, outputs = output )\n",
    "    model.compile( optimizer = 'sgd',\n",
    "                   loss      = 'binary_crossentropy', \n",
    "                   metrics   = [ 'accuracy' ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model_no_batchnorm = create_mnist_model()\n",
    "mnist_model_batchnorm    = create_mnist_model( True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: ImageNet Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batchnorm",
   "language": "python",
   "name": "batchnorm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
